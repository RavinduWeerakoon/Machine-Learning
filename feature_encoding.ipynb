{ "cells": [ { "cell_type": "markdown", "metadata": {}, "source": [ "### Objective \n", "\n", "Feature engineering is one of the most important aspects and it is the part where one should spend the most time on. The objective of this exercise is to demonstarte different types of feature encoding methods used in contests. It is very common to see categorical features in a dataset. \n", "\n", "So what is feature encoding? It is the process of transforming a categorical variable into a continuous variable and using them in the model. Lets start with basic and go to advanced methods.\n", "\n", "\n", "To be covered:\n", "* One Hot Encoding & Label Encoding \n", "* Frequency Encoding\n", "* Target Mean Encoding\n" ] }, { "cell_type": "code", "execution_count": 1, "metadata": { "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19", "_kg_hide-input": false, "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5" }, "outputs": [], "source": [ "## Loading packages\n", "import numpy as np\n", "import pandas as pd\n" ] }, { "cell_type": "code", "execution_count": 2, "metadata": { "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0", "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a" }, "outputs": [ { "data": { "text/html": [ "
\n", "\n", "\n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", "
PassengerId	Survived	Pclass	Name	Sex	Age	SibSp	Parch	Ticket	Fare	Cabin	Embarked
0	1	0	3	Braund, Mr. Owen Harris	male	22.0	1	0	A/5 21171	7.2500	NaN	S
1	2	1	1	Cumings, Mrs. John Bradley (Florence Briggs Th...	female	38.0	1	0	PC 17599	71.2833	C85	C
2	3	1	3	Heikkinen, Miss. Laina	female	26.0	0	0	STON/O2. 3101282	7.9250	NaN	S
3	4	1	1	Futrelle, Mrs. Jacques Heath (Lily May Peel)	female	35.0	1	0	113803	53.1000	C123	S
4	5	0	3	Allen, Mr. William Henry	male	35.0	0	0	373450	8.0500	NaN	S
\n", "
" ], "text/plain": [ " PassengerId Survived Pclass \\\n", "0 1 0 3 \n", "1 2 1 1 \n", "2 3 1 3 \n", "3 4 1 1 \n", "4 5 0 3 \n", "\n", " Name Sex Age SibSp \\\n", "0 Braund, Mr. Owen Harris male 22.0 1 \n", "1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 \n", "2 Heikkinen, Miss. Laina female 26.0 0 \n", "3 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 \n", "4 Allen, Mr. William Henry male 35.0 0 \n", "\n", " Parch Ticket Fare Cabin Embarked \n", "0 0 A/5 21171 7.2500 NaN S \n", "1 0 PC 17599 71.2833 C85 C \n", "2 0 STON/O2. 3101282 7.9250 NaN S \n", "3 0 113803 53.1000 C123 S \n", "4 0 373450 8.0500 NaN S " ] }, "execution_count": 2, "metadata": {}, "output_type": "execute_result" } ], "source": [ "## Loading dataset\n", "## Change the path to files\n", "train = pd.read_csv(\"train.csv\")\n", "test = pd.read_csv(\"test.csv\")\n", "\n", "## Glimpse throught the data\n", "train.head()\n" ] }, { "cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [ { "data": { "text/html": [ "
\n", "\n", "\n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", "
PassengerId	Survived	Pclass	Age	SibSp	Parch	Fare
count	891.000000	891.000000	891.000000	714.000000	891.000000	891.000000	891.000000
mean	446.000000	0.383838	2.308642	29.699118	0.523008	0.381594	32.204208
std	257.353842	0.486592	0.836071	14.526497	1.102743	0.806057	49.693429
min	1.000000	0.000000	1.000000	0.420000	0.000000	0.000000	0.000000
25%	223.500000	0.000000	2.000000	20.125000	0.000000	0.000000	7.910400
50%	446.000000	0.000000	3.000000	28.000000	0.000000	0.000000	14.454200
75%	668.500000	1.000000	3.000000	38.000000	1.000000	0.000000	31.000000
max	891.000000	1.000000	3.000000	80.000000	8.000000	6.000000	512.329200
\n", "
" ], "text/plain": [ " PassengerId Survived Pclass Age SibSp \\\n", "count 891.000000 891.000000 891.000000 714.000000 891.000000 \n", "mean 446.000000 0.383838 2.308642 29.699118 0.523008 \n", "std 257.353842 0.486592 0.836071 14.526497 1.102743 \n", "min 1.000000 0.000000 1.000000 0.420000 0.000000 \n", "25% 223.500000 0.000000 2.000000 20.125000 0.000000 \n", "50% 446.000000 0.000000 3.000000 28.000000 0.000000 \n", "75% 668.500000 1.000000 3.000000 38.000000 1.000000 \n", "max 891.000000 1.000000 3.000000 80.000000 8.000000 \n", "\n", " Parch Fare \n", "count 891.000000 891.000000 \n", "mean 0.381594 32.204208 \n", "std 0.806057 49.693429 \n", "min 0.000000 0.000000 \n", "25% 0.000000 7.910400 \n", "50% 0.000000 14.454200 \n", "75% 0.000000 31.000000 \n", "max 6.000000 512.329200 " ] }, "execution_count": 3, "metadata": {}, "output_type": "execute_result" } ], "source": [ "train.describe()" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "#### Before we jump in feature encoding, let's go ahead and remove unwanted variables like Cabin and Ticket." ] }, { "cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": [ "## Removing dummy variables\n", "train.drop(labels = [\"Cabin\", \"Ticket\"], axis = 1, inplace = True)\n", "test.drop(labels = [\"Cabin\", \"Ticket\"], axis = 1, inplace = True)\n" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "#### 1. Converting missing values to NaN.\n", "#### 2. Imputation with Median and Mode for \"Age\" and \"Embarked\"" ] }, { "cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": [ "## Fill missing values with NaN\n", "train = train.fillna(np.nan)\n", "test = test.fillna(np.nan)\n" ] }, { "cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [ { "data": { "text/plain": [ "PassengerId 0\n", "Survived 0\n", "Pclass 0\n", "Name 0\n", "Sex 0\n", "Age 177\n", "SibSp 0\n", "Parch 0\n", "Fare 0\n", "Embarked 2\n", "dtype: int64" ] }, "execution_count": 6, "metadata": {}, "output_type": "execute_result" } ], "source": [ "## Check for Null values\n", "train.isnull().sum()" ] }, { "cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": [ "## Missing Values Imputation\n", "train[\"Age\"].fillna(train[\"Age\"].median(), inplace = True)\n", "train[\"Embarked\"].fillna(\"S\", inplace = True)" ] }, { "cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": [ "## Lets create a variable called title from the name variable\n", "for name in train[\"Name\"]:\n", " train[\"Title\"] = train[\"Name\"].str.extract(\"([A-Za-z]+)\\.\",expand=True)\n", "\n", "title_replacements = {\"Mlle\": \"Other\", \"Major\": \"Other\", \"Col\": \"Other\", \"Sir\": \"Other\", \"Don\": \"Other\", \"Mme\": \"Other\",\n", " \"Jonkheer\": \"Other\", \"Lady\": \"Other\", \"Capt\": \"Other\", \"Countess\": \"Other\", \"Ms\": \"Other\", \"Dona\": \"Other\"}\n", "\n", "train.replace({\"Title\": title_replacements}, inplace=True)\n", "train.replace({\"Title\": title_replacements}, inplace=True)\n" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "### One Hot Encoding & Label Encoding\n", "\n", "Let's say we have ‘eggs’, ‘butter’ and ‘milk’ in a categorical variable\n", "\n", "* **One Hot Encoding** will produce three columns and the presence of a class will be represented in binary format. Three classes are separated out to three different features. The alogirithm is only worried about their presence/absence without making any assumptions of their relationship.\n", "* **Label Encoding** gives numerical aliases to the classes. So the resultant label enocded feature will have 0,1 and 2. The problem with this approach is that there is no relation between these three classes yet our alogirithm might consider them to be ordered (that is there is some relation between them) maybe 0<1<2 that is ‘eggs’<‘butter’<‘milk’.\n", "\n", "Depending on the variable we should either use one hot encoding or label encoding.\n" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "#### One Hot Encoding" ] }, { "cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [ { "data": { "text/html": [ "
\n", "\n", "\n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", "
Embarked_C	Embarked_Q	Embarked_S	Pclass_1	Pclass_2	Pclass_3	Title_Dr	Title_Master	Title_Miss	Title_Mr	Title_Mrs	Title_Other	Title_Rev
0	0	0	1	0	0	1	0	0	0	1	0	0	0
1	1	0	0	1	0	0	0	0	0	0	1	0	0
2	0	0	1	0	0	1	0	0	1	0	0	0	0
3	0	0	1	1	0	0	0	0	0	0	1	0	0
4	0	0	1	0	0	1	0	0	0	1	0	0	0
\n", "
" ], "text/plain": [ " Embarked_C Embarked_Q Embarked_S Pclass_1 Pclass_2 Pclass_3 Title_Dr \\\n", "0 0 0 1 0 0 1 0 \n", "1 1 0 0 1 0 0 0 \n", "2 0 0 1 0 0 1 0 \n", "3 0 0 1 1 0 0 0 \n", "4 0 0 1 0 0 1 0 \n", "\n", " Title_Master Title_Miss Title_Mr Title_Mrs Title_Other Title_Rev \n", "0 0 0 1 0 0 0 \n", "1 0 0 0 1 0 0 \n", "2 0 1 0 0 0 0 \n", "3 0 0 0 1 0 0 \n", "4 0 0 1 0 0 0 " ] }, "execution_count": 9, "metadata": {}, "output_type": "execute_result" } ], "source": [ "## subset categorical variables which you want to encode\n", "x = train[['Embarked','Pclass','Title']]\n", "\n", "x = pd.get_dummies(x, columns=['Embarked','Pclass','Title'], drop_first=False)\n", "x.head()\n" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "#### We see that all the sub categories in a categorical variable have been converted into binary flags. This type of feature encoding is one hot encoding." ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "#### Label encoding" ] }, { "cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": [ "## subset categorical variables which you want to encode\n", "x = train[['Embarked','Pclass','Title']]\n", "\n", "## Write your code here" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "#### We see that all the subcategories in the categorical variable have been given numbered aliases. " ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "### Frequency Encoding\n", "\n", "* Step 1 : Select a categorical variable you would like to transform.\n", "* Step 2 : Group by the categorical variable and obtain counts of each category.\n", "* Step 3 : Join it back with the train dataset.\n" ] }, { "cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [ { "data": { "text/html": [ "
\n", "\n", "\n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", "
Title	Freq_Encoded_Title
0	Dr	7
1	Master	40
2	Miss	182
3	Mr	517
4	Mrs	125
\n", "
" ], "text/plain": [ " Title Freq_Encoded_Title\n", "0 Dr 7\n", "1 Master 40\n", "2 Miss 182\n", "3 Mr 517\n", "4 Mrs 125" ] }, "execution_count": 11, "metadata": {}, "output_type": "execute_result" } ], "source": [ "## sample train dataset\n", "sample_train = train[['Embarked','Pclass','Title']]\n", "\n", "## Frequency Encoding title variable\n", "y = sample_train.groupby(['Title']).size().reset_index()\n", "y.columns = ['Title', 'Freq_Encoded_Title']\n", "y.head()\n" ] }, { "cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [ { "data": { "text/html": [ "
\n", "\n", "\n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", "
Embarked	Pclass	Title	Freq_Encoded_Title
0	S	3	Mr	517
1	C	1	Mrs	125
2	S	3	Miss	182
3	S	1	Mrs	125
4	S	3	Mr	517
\n", "
" ], "text/plain": [ " Embarked Pclass Title Freq_Encoded_Title\n", "0 S 3 Mr 517\n", "1 C 1 Mrs 125\n", "2 S 3 Miss 182\n", "3 S 1 Mrs 125\n", "4 S 3 Mr 517" ] }, "execution_count": 12, "metadata": {}, "output_type": "execute_result" } ], "source": [ "sample_train = pd.merge(sample_train,y,on = 'Title',how = 'left')\n", "sample_train.head()\n" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "#### We see that all the subcategories in the categorical variable have been given the total number of occurance for that specific category. " ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "### Mean Encoding \n", "**Survived** is our dependent variable (DV), so let's look at how we can extract features from it. The following steps are used in **Mean encoding**,\n", "\n", "* Step 1 : Select a categorical variable you would like to transform.\n", "* Step 2 : Group by the categorical variable and obtain aggregated sum over \"survived\" variable.\n", "(total number of 1's for each category in DV)\n", "* Step 3 : Group by the categorical variable and obtain aggregated count over \"survived\" variable.\n", "* Step 4 : Divide the step 2 / step 3 results and join it back with the train.\n" ] }, { "cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [ { "data": { "text/html": [ "
\n", "\n", "\n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", "
Title	Title_Survived_sum	Title_Survived_count	Target_Encoded_over_Title
0	Dr	3	7	0.428571
1	Master	23	40	0.575000
2	Miss	127	182	0.697802
3	Mr	81	517	0.156673
4	Mrs	99	125	0.792000
\n", "
" ], "text/plain": [ " Title ... Target_Encoded_over_Title\n", "0 Dr ... 0.428571\n", "1 Master ... 0.575000\n", "2 Miss ... 0.697802\n", "3 Mr ... 0.156673\n", "4 Mrs ... 0.792000\n", "\n", "[5 rows x 4 columns]" ] }, "execution_count": 13, "metadata": {}, "output_type": "execute_result" } ], "source": [ "sample_train = train[['Title','Survived']]\n", "\n", "## Mean encoding \n", "x = sample_train.groupby(['Title'])['Survived'].sum().reset_index()\n", "x = x.rename(columns={\"Survived\" : \"Title_Survived_sum\"})\n", "\n", "y = sample_train.groupby(['Title'])['Survived'].count().reset_index()\n", "y = y.rename(columns={\"Survived\" : \"Title_Survived_count\"})\n", "\n", "z = pd.merge(x,y,on = 'Title',how = 'inner')\n", "z['Target_Encoded_over_Title'] = z['Title_Survived_sum']/z['Title_Survived_count']\n", "z.head()\n" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "#### We see that all the subcategories in the categorical variable are represented as the survival probabilty occuring in that specific category." ] }, { "cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [ { "data": { "text/html": [ "
\n", "\n", "\n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", "
Title	Survived	Target_Encoded_over_Title
0	Mr	0	0.156673
1	Mrs	1	0.792000
2	Miss	1	0.697802
3	Mrs	1	0.792000
4	Mr	0	0.156673
\n", "
" ], "text/plain": [ " Title Survived Target_Encoded_over_Title\n", "0 Mr 0 0.156673\n", "1 Mrs 1 0.792000\n", "2 Miss 1 0.697802\n", "3 Mrs 1 0.792000\n", "4 Mr 0 0.156673" ] }, "execution_count": 14, "metadata": {}, "output_type": "execute_result" } ], "source": [ "## Joining this back with the sample_train dataset\n", "\n", "z = z[['Title','Target_Encoded_over_Title']]\n", "\n", "sample_train = pd.merge(sample_train,z,on = 'Title',how = 'left')\n", "sample_train.head()\n" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "What will you do if you want to mean encode a categorical variable using a **continuous variable** instead of a **dichotomous/binary variable**? How will you use mean encoding? There are two methods which can be used for mean encoding continuous variables : \n", "\n", "1. Direct method\n", "2. k-fold method " ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "#### Direct Method\n", "* Step 1 : Select a categorical variable you would like to transform\n", "* Step 2 : Select a continuous variable variable.\n", "* Step 3 : Group by the categorical variable and obtain the aggregated mean over the numeric variable." ] }, { "cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [ { "data": { "text/html": [ "
\n", "\n", "\n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", "
Title	Title_Mean_Encoded
0	Dr	49.168457
1	Master	34.703125
2	Miss	43.797873
3	Mr	24.441560
4	Mrs	45.138533
\n", "
" ], "text/plain": [ " Title Title_Mean_Encoded\n", "0 Dr 49.168457\n", "1 Master 34.703125\n", "2 Miss 43.797873\n", "3 Mr 24.441560\n", "4 Mrs 45.138533" ] }, "execution_count": 15, "metadata": {}, "output_type": "execute_result" } ], "source": [ "## Direct Method\n", "## TYPE 1\n", "## Selecting title (categorical) and Fare (numeric) from the train dataset\n", "\n", "sample_train = train[['Title','Fare']]\n", "\n", "## Mean encoding \n", "x = sample_train.groupby(['Title'])['Fare'].mean().reset_index()\n", "x = x.rename(columns={\"Fare\" : \"Title\" +\"_Mean_Encoded\"})\n", "x.head()\n" ] }, { "cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [ { "data": { "text/html": [ "
\n", "\n", "\n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", " \n", "
Title	Fare	Title_Mean_Encoded
0	Mr	7.2500	24.441560
1	Mrs	71.2833	45.138533
2	Miss	7.9250	43.797873
3	Mrs	53.1000	45.138533
4	Mr	8.0500	24.441560
\n", "
" ], "text/plain": [ " Title Fare Title_Mean_Encoded\n", "0 Mr 7.2500 24.441560\n", "1 Mrs 71.2833 45.138533\n", "2 Miss 7.9250 43.797873\n", "3 Mrs 53.1000 45.138533\n", "4 Mr 8.0500 24.441560" ] }, "execution_count": 16, "metadata": {}, "output_type": "execute_result" } ], "source": [ "## Joining this back with the sample_train dataset\n", "\n", "sample_train = pd.merge(sample_train,x,on = 'Title',how = 'left')\n", "sample_train.head()\n" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "#### We see that each title is encoded into the mean of Ticket Fare. This is a popularly used feature encoding technique in kaggle competitions. \n", "\n", "#### But why are these encodings better ?\n", "\n", "* Mean encoding can embody the target in the label whereas label encoding has no correlation with the target.\n", "* In case of large number of features, mean encoding could prove to be a much simpler alternative.\n", "* A histogram of predictions using label & mean encoding show that mean encoding tend to group the classes together whereas the grouping is random in case of label encoding\n", "\n", "![](https://cdn-images-1.medium.com/max/800/1*qwooYKx8rU6h1VDnUCgsNg.png)\n", "\n", "\n", "* Even though it looks like mean encoding is Superman, it’s kryptonite is overfitting. The fact that we use target classes to encode for our training labels may leak data about the predictions causing the encoding to become biased. Well we can avoid this by Regularizing. \n", "\n", "#### Now let's look at how we can reduce this bias." ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "#### What is k-fold cross validation?\n", "\n", "Cross-validation is primarily used in machine learning to estimate the skill of a machine learning model on unseen data. Let's say the value of k is 5. You break the train dataset into 5 parts hold out one part as test and run a model using the other 4 parts as train. This is iteratively done such that the model trains through all combinations of the dataset. (refer image below)\n", "\n", "#### How can we use this for mean encoding ?\n", "Since K-fold strategy holds out some data, it reduces the bias we discussed about earlier and applying the direct method over k-folds will be the best way to do feature encoding. \n" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "![](https://cdn-images-1.medium.com/max/1600/1*me-aJdjnt3ivwAurYkB7PA.png)" ] } ], "metadata": { "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" }, "language_info": { "codemirror_mode": { "name": "ipython", "version": 3 }, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4" } }, "nbformat": 4, "nbformat_minor": 1 }
